{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 용어 정리\n",
    "### 1. basic\n",
    "\n",
    "\n",
    "- `num_iterations`: `n_estimators`, `num_boost_round` \n",
    "- `learning_rate`: `eta`\n",
    "- `nthread`: `n_jobs`\n",
    "- `feature_fraction`: 0.8 feature fraction means LightGBM will select 80% of parameters randomly in each iteration for building trees.\n",
    "- `device_type`: `cpu`, `gpu`\n",
    "- `early_stopping_round`: Model will stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds. This will reduce excessive iterations.\n",
    "- `max_cat_group`: When the number of category is large, finding the split point on it is easily over-fitting.\n",
    "\n",
    "\n",
    "### 2. core\n",
    "\n",
    "\n",
    "- `num_leaves`: control the complexity of the tree model. val should be less than (equal to) $2^{max_depth}$\n",
    "- `min_data_in_leaf`: set it to a large value to avoid growing too deep a tree, but may cause under_fiiting. setting it to hundreds or thousands is enough\n",
    "- `max_depth`\n",
    "\n",
    "\n",
    "### 3. Faster speed\n",
    "\n",
    "\n",
    "- Use bagging by setting `bagging_fraction` and `bagging_freq`\n",
    "- Use feature sub-sampling by setting `feature_fraction`\n",
    "- Use small `max_bin`\n",
    "- Use save_binary to speed up data loading in future learning\n",
    "- Use parallel learning\n",
    "\n",
    "\n",
    "### 4. Better accuracy\n",
    "\n",
    "\n",
    "- Use large `max_bin`\n",
    "- Use samll `learning_rate` with large `num_iterations`\n",
    "- Use large `num_leaves`(risk over-fitting)\n",
    "- Try `dart`\n",
    "- Use categorical feature directly\n",
    "\n",
    "\n",
    "### Deal with over-ftting\n",
    "- Use small `max_bin`\n",
    "- Use small `num_leaves`\n",
    "- Use `min_data_in_leaf`(large num) and `min_sum_hessian_in_leaf`\n",
    "- Use bagging by setting `bagging_fraction` and `bagging_freq`\n",
    "- Try `lambda_l1`, `lambda_l2` and `min_gain_to_split` to regularization\n",
    "\n",
    "(https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn tools for model training and assesment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score)\n",
    "\n",
    "df_train = pd.read_csv(\"binary.train\", header=None, sep='\\t')\n",
    "df_test = pd.read_csv(\"binary.test\", header=None, sep='\\t')\n",
    "\n",
    "y_train = df_train[0].values\n",
    "y_test = df_test[0].values\n",
    "X_train = df_train.drop(0, axis=1).values\n",
    "X_test = df_test.drop(0, axis=1).values\n",
    "\n",
    "\n",
    "# set Dataset\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss', 'auc'},\n",
    "    'metric_freq': 1,\n",
    "    'is_training_metric': True,\n",
    "    'max_bin': 255,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 63,\n",
    "    'tree_learner': 'serial',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_sum_hessian_in_leaf': 5,\n",
    "    'is_enable_sparse': True,\n",
    "    'use_two_round_loading': False,\n",
    "    'is_save_binary_file': False,\n",
    "    'output_model': 'LightGBM_model.txt',\n",
    "    'num_machines': 1,\n",
    "    'local_listen_port': 12400,\n",
    "    'machine_list_file': 'mlist.txt',\n",
    "    'verbose': 0,\n",
    "    'subsample_for_bin': 200000,\n",
    "    'min_child_samples': 20,\n",
    "    'min_child_weight': 0.001,\n",
    "    'min_split_gain': 0.0,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'reg_alpha': 0.0,\n",
    "    'reg_lambda': 0.0\n",
    "}\n",
    "\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                valid_sets=lgb_eval)\n",
    "\n",
    "mdl = lgb.LGBMClassifier(\n",
    "    task = params['task'],\n",
    "    metric = params['metric'],\n",
    "    metric_freq = params['metric_freq'],\n",
    "    is_training_metric = params['is_training_metric'],\n",
    "    max_bin = params['max_bin'],\n",
    "    tree_learner = params['tree_learner'],\n",
    "    feature_fraction = params['feature_fraction'],\n",
    "    bagging_fraction = params['bagging_fraction'],\n",
    "    bagging_freq = params['bagging_freq'],\n",
    "    min_data_in_leaf = params['min_data_in_leaf'],\n",
    "    min_sum_hessian_in_leaf = params['min_sum_hessian_in_leaf'],\n",
    "    is_enable_sparse = params['is_enable_sparse'],\n",
    "    use_two_round_loading = params['use_two_round_loading'],\n",
    "    is_save_binary_file = params['is_save_binary_file'],\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "scoring = {'AUC': 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_param\n",
    "gridParams = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5],\n",
    "    'max_dpth': [3, 6]\n",
    "    'num_leaves': [8, 32],\n",
    "    'boosting_type' : ['gbdt', 'dart'],\n",
    "}\n",
    "\n",
    "# Create the grid\n",
    "grid = GridSearchCV(mdl, gridParams, verbose=2, cv=5, scoring=scoring, n_jobs=-1, refit='AUC')\n",
    "# Run the grid\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters found by grid search are:', grid.best_params_)\n",
    "print('Best score found by grid search is:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lightgbm.Dataset(X_train, label=y_train)\n",
    "dtest = lightgbm.Dataset(X_test, label=y_test)\n",
    "\n",
    "num_boost_round = 1000\n",
    "learning_rate = 0.02\n",
    "\n",
    "params = {'objective': 'multiclass',\n",
    "          'boosting_type': 'gbdt',\n",
    "          'max_depth': -1,\n",
    "          'nthread': 4,\n",
    "          'metric': 'multi_logloss',\n",
    "          'num_class': 38,\n",
    "          'learning_rate': learning_rate,\n",
    "          }\n",
    "\n",
    "lightgbm_model = lightgbm.train(params=params,\n",
    "                                train_set=dtrain,\n",
    "                                valid_sets=[dtrain, dtest],\n",
    "                                num_boost_round=num_boost_round,\n",
    "                                early_stopping_rounds=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
