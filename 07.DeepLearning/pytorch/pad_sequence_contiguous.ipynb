{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. pad_sequence\n",
    "- max seq_len에 맞춰서 padding\n",
    "- link: https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "\n",
    "### param QA\n",
    "- batch_fifst: https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402\n",
    "    - `False`인 경우\n",
    "        - input: seq_len, batch_size, features\n",
    "        - output: seq_len, batch_size, features\n",
    "    - `True`인 경우\n",
    "        - input: batch_size, seq_len, features\n",
    "        - output: batch_size, seq_len, features\n",
    "\n",
    "# 2. pack_padded_sequence\n",
    "- pad=0을 계산하지 않기 위한 작업\n",
    "- return:\n",
    "    - elems: 데이터를 flatten한 값\n",
    "    - 각 time step에서 훈련할 데이터 갯수(pad 데이터 제외) \n",
    "- 참고:https://github.com/HarshTrivedi/packing-unpacking-pytorch-minimal-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [3, 4, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# pad_sequence\n",
    "a = [torch.tensor([1,2,3]), torch.tensor([3, 4])]\n",
    "b = torch.nn.utils.rnn.pad_sequence(a, batch_first=True)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack_padded_sequence\n",
    "seq_len = [len(t) for t in a]\n",
    "packed_input = torch.nn.utils.rnn.pack_padded_sequence(b, batch_first=True,\\\n",
    "                                        lengths=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 2, 4, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_input.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 마지막에 train set batch size 1인 이유는, \n",
    "# b에서 볼 수 있듯, 데이터가 3, pad=0 이므로 \n",
    "# 0 제외한 갯수 \n",
    "packed_input.batch_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. contiguous, repea, copy_\n",
    "- link: https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n",
    "- link2: https://stackoverflow.com/questions/48915810/pytorch-contiguous\n",
    "- 관련 에러: view operation runtime error\n",
    "    - view는 discontiguous에서는 작동 안됨\n",
    "    - ` invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view().`\n",
    "- stride: `Stride is the jump necessary to go from one element to the next one in the specified dimension`\n",
    "    - 다음 row로 가려면; 다음 column으로 가려면 메모리 몇개를 skip해야 하는가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]]), (3, 1))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row: 0 -> 3가려면 3개 메모리 skip\n",
    "# col: 0 -> 1가려면 1개 메모리 skip\n",
    "# 즉 메모리가 0,1,2,3,4,5,..이렇게 저장중\n",
    "x = torch.arange(12).view(4,3)\n",
    "x, x.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- view 에러 케이스\n",
    "    - transpose 후 메모리는 contiguous하지 않음\n",
    "    - 그상태로 flatten 시도하면 에러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "(1, 3)\n",
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "tensor([[ 0,  3,  6,  9],\n",
      "        [ 1,  4,  7, 10],\n",
      "        [ 2,  5,  8, 11]])\n",
      "(4, 1)\n",
      "tensor([ 0,  3,  6,  9,  1,  4,  7, 10,  2,  5,  8, 11])\n"
     ]
    }
   ],
   "source": [
    "y = x.t()\n",
    "\n",
    "print(y.is_contiguous()) # not contiguous\n",
    "print(y.stride()) # stride 바뀜!!\n",
    "\n",
    "# error\n",
    "try:\n",
    "    y.view(-1)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "# solution\n",
    "y = y.contiguous()\n",
    "print(y)\n",
    "print(y.stride())\n",
    "print(y.view(-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_conda",
   "language": "python",
   "name": "torch_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
