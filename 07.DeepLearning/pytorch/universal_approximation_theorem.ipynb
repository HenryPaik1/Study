{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7ea36e85c0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD7CAYAAAB+B7/XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3xcV5338c/MaIp6r5ZcZR0XuY4dO3acOJUEHBISSgJLQtgAIcCyhX3g2eehPGSzQCCEXcguLBBgIU5IAKeQkOIkjp04ju1xlcuxZVu2ZclWl6w2mvb8MTNGjpvKHY3mzu/9evk11pR7f1cafXXm3HPPsYRCIYQQQpiHNd4FCCGEMJYEuxBCmIwEuxBCmIwEuxBCmIwEuxBCmExKvAvweDxOYDHQCATiXI4QQiQKG1AKbHG73d7BD8Q92AmH+oZ4FyGEEAlqBfDW4DvGQ7A3AlRVVeFwOIb94pqaGqqrqw0vajyTY04OcszJYaTHPDAwwIEDByCSoYONh2APADgcDpxO54g2MNLXJTI55uQgx5wcRnnM53Rhy8lTIYQwGQl2IYQwGQl2IYQwGQl2IYQwGQl2IYQwGQl2IYQwmfEw3FEI0xrwBWjp6KO5vY/Wrn66egbo6vFGbsP/vAN++gcCeH0BvJFbvz9IKBTC8uSJs7bnsNtwOmy4HDackf+nuexkpzvJznCQleEgJ8NJbqaLorw0inJTSXPZ43T0Il4MC3allAt4BLgO6Afe0Vp/1qjtCzFe9Xn9nGjqpr7pNPVN3dQ3d9PU1ktzRx8dp73nPN9qtZCV7iAr3UFmmoOcTBfOQUHtcqSQYrNw8uRJSkpKznrtmfCP/AHo9/rp6ffR2NJDV4+XPu+5s3JkptkpykujOC+NiuJMJpVkMakkk7LCDFJs8qHdjIxssT9EONCrtNYhpVSxgdsWIu5CoRCtnf3U1ndQW9/BofpO6hq7aOnoO/Mcq9VCSSREp07IpjAnlcLcVApz0sjLdpGd7iDNZcdqtVxyfx5PH273rGHVOOAL0Nk9QFtXH03tfTS19XKqvZemtl7qGrrYtLuRYGTRtBSbhfKiTNSkXKom5qIm5VJRlDmk2sT4ZkiwK6UygLuAcq11CEBrfcqIbQsRLz5/gIPHO9hzuJW9R9qoPd5BR3e4BW61QEVxJtXT8qkoyqS8KIPyogxKCzKwp8SvFeyw28J/SHJTUZPOfXzAF6C+qZtjJ7s4evI0h0908vbOBl7edBSAVGcKs6bkMb+qkPlVRUwqycRikaBPNEa12KcBrcA3lVJXA93A/9Vav3XxlwkxfgQCQfYfbWf7gSb2HG7lwNF2BvxBACqKM3DPLKKyPIfK8hwml2XhciTeKSqH3cbUCdlMnZB95r5QKERDSw/6aBv769rZVdvCL5/bA+whJ9PJ/OmFLK0uZeGMIlKdiXfMychixGLWSik3sBX4hNZ6tVJqCfA8UKm17rrYaz0ez2TgyKiLEGIEunoD1Db2U9vQz6GT/Xh9ISwWKMm1M6nQyaQiJxMLHaS7bPEudUx19vg5fNLL4ZP91J700ucNkmKDaaUuZlWkospTcdmlf36cmOJ2u+sG32HUn9+jgB94AkBr/a5SqgWoIhz4l1RdXT2iiXA8Hg9ut3vYr0tkcsyjc7K1h427GnhrZwMHj3cAkJfl4soFFbhnFDOvqpCM1PiPJIn3z/mayG0gEGTvkTY27m7gnd2NrHmnHaeji2VzSrnusolUTy0wrF8+3sccDyM9Zq/XS01NzXkfMyTYtdYtSqk3gOuBV5RSVUARUGvE9oUYrab2Xt7cVs/buxo4VN8JQGVFDne9fyaLZhYzuTRL+pIvwGazMqeygDmVBXzmljnoo+28tvUYG3ac4A1PPcV5adx0+WTet3QSGWnDn3o7GQWDIZ558xBr1tVy9zW5hm/fyA6z+4DHlFIPAz7gk1rrDgO3L8SweH0B3tndyGubj7GztplQCNTEXO5ZNZvl88oozkuLd4kJx2q1MHNKHjOn5HHvLdVs2t3IK+8e49cv7OWJVzXXLqrgg1dOY0JhRrxLHbc6u7386MntbN13isvnlJKVZnyXlmHBrrU+DKw0antCjNSRhk5e3FjH+u319Pb7KcpL487rFVcvqqAkPz3e5ZmGy5HCSncFK90VHGno5Nn1h3jl3WP85Z06rlpQzh03KAn49zjS0Mm/PvYubV1e7rttLu9fNplt27YZvh85xS1MIRAMsXlPI89tOEzNoVYcdhvL5xrfByzOb0pZNn9/x0Lufv8snl1/iD+/fYT12+tZ6a7gzhuU/EEF3t7ZwCNPbiMj1c5DX7qC6RXGd8FESbCLhNbv9fPSpjqe33CYpvY+CnNTuWfVLK5fMolM6e8dc7lZLj61aja3XDWNP71Ry4tvH2HDjhN8aGUlH7lmOq4kHC4ZCoV48tUDrH55PzMm5fIvn7qM3CxXTPeZfN9lYQo9fT5eePsIz7x5iNO9A1RPy+feW6q5bFYJNrlMPu5yM1387QerufWqafz6hb08tfYAr205xqdvns2K+ROS5kR1IBjiZ2t28ZeNdVyzqIIvfmQe9pTYD52VYBcJpbffx5p1h3h+wyF6+v0smlnMx66rYsbkvHiXJs4jPzuVf/q4m5sun8x/P7Ob7//Ow5vbTvCFj8wjL8at1njz+QP8cPU23trZwO1XV3L3B2aN2R80CXaREPyBEM9tOMTvXz1AV88Al88p5aPXVVFZnhPv0sQQzJqSz8NfvornNxzmty/u5f6HXudzH5rDyoXlpmy9D/gCPPirzWzTTdyzaja3XV05pvuXYBfjWigU4q2dDfz8hZO0d59gbmUB96yaTWWFBHqisVkt3HrVNBbPKubfn9zOD1dvY7tu4v7b55mq793nD/Kd32xhm27iSx+dzw1LzjNpT4yZ57spTOf4qdP89E+72FXbQlGOnW99ZjELVZEpW3jJZEJhBt/5whU8tfYAT7yyn9r6Dr5612ImlWTFu7RR8weCPPTbLWzdd4r7PzwvLqEOEuxiHOof8PPU2gOsWVeL05HC52+fS6G9FfcMmQnaLGxWC3feoJg1OY8fPO7hn/9jPf/rk4tJ5D/ZoVCIHz+1g001J/nsrXO46fLJcasloYP9aGMXL25tZ9vx3aTYrKSkWEmxWXGkWElPtZORZicj1U5GqoOMNDv52a4xOSMtRm73oRb+/cntnGrr5ZpFFdyzajY5mU48nrZ4lyZiYF5VIT/6x6v49i/f5YFfbuJGdw6JOlXME69oXt96nI+/bwY3r5ga11oSOtjrm7vZc6yPPceO4QuE8AeCBIMXn60yJ8NJfo6LguzwnNXlhRmUF2VSXpxBXpZLPubHidcX4H9e3Mtz6w9TWpDOd+5fTvW0gniXJcZAfnYq3/3CFXz/d1t5cespUjP3jOkIEiO8tuUYT7yiuXZxBXdcXxXvchI72JfPLcPlazxrZrRAMITPF6Cn30d3r4/uPh+newc43TNAa1c/LR19tHT0cbK1h121zWctJZbqTGFyaRbTK3LC/ybmUpqfLlctxljt8Q4eXu2hvqmbVcuncPcHZpnqZJq4tFRnCv/nniX8689e449v1OLzB7n3luqECPcDx9r5ydM7mDe9gC98eP64qNl0vz02qwWbMwWXM4X87NSLPjcUCtHW1R9ep/LUaY43dXP4RCcvbTrKcxsOA5Ceaqd6aj5zKwuYN72QibKijGFCoRAvvn2EXzy3h5wMBw987nLmVxXFuywRJzarhQ8szqGsrJjn1h/GFwjy+dvmjuvft85uL9/9ny3kZbn46l2L47p61mCmC/bhsFgs5Genkp+dyrzphWfuDwSCHDt1moPHO9BH29ld28K7e04C4a6chTOKWFpdygJVmJCr6IwHvf0+Hn16J+t3nGDRzGL+8eMLZQoAgcVi4d4PVmO3WfnjG7VkpNq56/3DW/d1rASCIX64ehvtXV4e+tIV4+r9K6l0HjablSll2Uwpyz4zXKmprZddtc3sOBAO+de3Hsdht+GeUcSK+RNYMrsEh11OzA5FQ0s3D/zyXRqau7nr/TO5/erp0t0lzrBYLNz9gVn09Pt5+rWD5GQ6+eCKafEu6xxr1tWyTTdx/4fnxXRCr5GQYB+iorw0rrtsEtddNgl/IMieQ628U9PIO7vD/zJS7Vy5YALXLp7I9Iqccf3xMZ5qDrXwb7/eDFh44L5lzK0svORrRPKxWCzcd9tcOru9/PyZGvKzUlk+ryzeZZ1R19jF4y/tZ9ncUm5cGp+x6hcjwT4CKTYr86oKmVdVyGduncOug828tuU4azcf48WNdVSWZ3PLldO4Yv4EUmRCqjNe33qMHz+1g+K8dL5x7xLKCmSubnFhNquFr3zCzf/5r7f50ZPbKC/KYFJp/C9i8vmDPLI6PP3u/bfPG5eNOEmdUbJZLSxQRXzlb9z8z7du5P7b59I/EODh1du498FX+cPrB+nt98W7zLhbs66WR57Yzqwp+fzg71ZIqIshcdhtfO3uxaQ6U3jw15vp7ov/79If3zjI4YZOvviReWRnDH+d5rFgeLArpb6plAoppaqN3vZ4l55q56ZlU3j0n6/hm/cupaIok9+8sJd7H1zLn96opX/AH+8Sx1woFOLxl/bz2PN7WD6vjG995nJZF1MMS352Kl+7ezFNbb38+KnthEIXv1Yllk629vD02gPh82rVpXGr41IMDXal1EJgKXDMyO0mGqvVwqKZxTxw3zIe/vKVVJZn86s/7+Fz31nLK+8eveRFVGYRCoX45XN7ePJVzXWLJ/LPf7No3AwHE4ll1pR8PnnTTDbuauQNT33c6vjFszVYrRY+ffPsuNUwFIb9limlnMCjwP1AciTXEFRNzOXbn1vGd+5fTnFeOj9+agdf+Y/1HDjWHu/SYioUCvGbF/by7PpD3LxiKl/66HxsMvJFjMKtKyuZPTWfn63ZRVNb75jvf8vek7y75yR33qAoyLn4NTLxZjHqY41S6nvAMa31o0qpOmCV1rrmUq/zeDyTgSOGFDHOhUIhdtX18ur2Trr7gyyens71C7JxmLAVu35PF6/v7GLR9HQ+sEhGCQljtHf7+a8XT1Fe4OCTVxeM2fsqGAzxny+eIgR8/qZiUmzj6v08xe121w2+w5BRMUqpy4HFwNdGuo3q6mqczuGfiPB4PGdNKTDeLVoEd6zy8fhL+3n+rcPUt8M/3LGQmVOGvgLQeD/mF946zOs761m5sJx/uHOhIWPUx/sxx4Ic8/l5bYf56Zrd9KWUsmL+hDGp6/Wtx2jpOsHX7l7MkrnGDrsc6c/Z6/VSU3P+trNRTcWrgBnAkUhrvRx4WSl1g0HbN5U0l53P3DqHBz+/nEAwxNce3cDv/rKPgAn63jfvPcnPntnNktklfPmOBXLhkTDcjcumMK08m188WzMmI858/iCPv6ypLM9m2Zzxe8J0MEOCXWv9Xa11mdZ6stZ6MlAPvE9r/YoR2zerOdMK+PE/reSaRRP5/doD/L+fv0NXz0C8yxqxIw2d/OB3W5k6IZuvfMItY/hFTNisFj5/21zauvp5+rWDMd/f2s1HaWrr5ZM3Jc6Mk/KbF2dpLjtfvmMBX/zIfHYfauXvH1nHkYbOeJc1bB2nvTzw2LukOu18/dNLZHZGEVNqUh4r3eU8t+EwbV39MdtPIBhizZuHqJqYwwKVOFdJxyTYIy33S544FX/1vqWT+N4XryAYDPG1R99i58HmeJc0ZMFgiIdXe+g87eXrn15yyVk1hTDCx2+YQSAQ5Km1B2K2j817Gmls6eFDKysTprUO0mIfV6om5vL9L11JQU4q3/r5O6zfHr/xusPx9OsH2HGgmc9+aI4sMi3GTGlBOtcvmcTLm+o42doTk32sWXeI4rw0Lh/HFyOdjwT7OFOYm8r3vriCGZPzePhxD+u2je9w332ohdUv7WflwvK4Ldwrkld4tSILa9bVGr5tfbSNfXVt3HLlNGwJdr4osapNEhmpdr5571JmTy3gkdWecdty7+338cPV2ygtSOf+D4/PyZCEueVnp7JyYTlrtxw3fODBy5uO4nLYuHZxhaHbHQsS7OOUy5HCN/52CTOn5PPw4x427z0Z75LO8djze2jr7OMf7lxIqpwsFXFy68ppDPgC/OUd465z7O33sX7HCa5cUE6ay27YdseKBPs45nKm8M17lzJ1QjYP/XYrtcc74l3SGdt0Ey9vOsqHVlaiJg394iohjDapJIuFM4p44a0j+ANBQ7a5YccJvAMBblgy0ZDtjTUJ9nEu1ZnC1/92KVnpDr79y000t/fFuyS8vgCP/mEn5UUZfPx9M+JdjhC8//LJtJ/24tl3ypDtrd18jIklmVRNHF8rIw2VBHsCyMty8c17l9I/EOB7v92CPxDfK1TXrKulqa2X+2+fJ8sBinFh0cxicjOdvLp59BPLNrX3sv9oOysXlifseSMJ9gQxqSSLL39sAfpoO2t3xO8Cpqb2Xp5+7SDL55Uxp7IgbnUIMZjNZuWaRRVs2Xdq1BcsbdzVAMAV88ZmHppYkGBPIMvnlXHziqls0t1sqmmMSw2/en4PAJ9eNb7noxbJ5/olkwgGQ7w5yiHCb+1sYOqEbEoL0g2qbOxJsCeYe1bNpjTXzqNP7xzzeWUOHm/nrZ0N3LaykqK8tDHdtxCXMqEwg2nl2by9s2HE22hu70MfbeeKcbRw9khIsCcYe4qVW5bm0t03wM/W7BrTfT/+0n4y0+x8aOW0Md2vEEO1fG4Z+lg7Te0jW4hj857wJ+FlBk/NO9Yk2BNQSa6Dj12vWL/9BFvGaHz7/ro2PPubuO3q6Qk5rlckh+WRQN64a2Rdldt0MyX5aUwoTOzF1iXYE9SHr5lOeVEGv3i2Bp/fmLG7F7P65f3kZDhZtXxKzPclxEiVFWYwuTRrRA0enz/IrtpmFqqiGFQ2tiTYE1SKzcq9t1TT0NLD8xsOx3RfRxo62X6gmQ9eOVWm4xXj3kJVxN4jrfR5/cN63b66VvoHAhLsIr7cM4pZNLOYJ1/VdHZ7Y7afZ9cfwumwcePlk2O2DyGMsnBGEf5AiN21LcN63XbdjM1qMcUwXgn2BPfpm2fTP+CPyex2AO1d/by57QTXLqogM80Rk30IYaRZU/JwOmx49g/vKtQ9h1uprMgxxTkkoxazzgd+C0wDvEAt8DmtdeKsFpGgKoozuXJ+OX9++wi3XlVJTubwFwS/mBc31hEIBvnglTISRiQGe4qNOdMK2HFg6PEz4Atw8HgHN6+YGsPKxo5RLfYQ8JDWWmmt5wKHgO8atG1xCXfcUIXPF+CZN41ttQeCIdZuPsoCVZTwowREcpkzLZ+Glh46Tg+ti7K2vgN/IMisKeaY0M6oxazbtNbrBt21CZBVF8ZIeVEmy+aW8dI7dcM+YXQxOw8009LZz/WXJeYMdyJ5zZgcDuh9dW1Dev7eI+HnzZwswX5eSikr8HngOaO3LS7sliun0dPv5/Uto58EKWrtlmNkptlZMrvEsG0KMRYqy3NIsVmHHOz7jrQxoTCD7AxjuzLjJRZj134MdAM/Gc6LampGvva1x+MZ8WsT1XuPORQKMSHfztNr91HkasM6ylnp+gaCbNzVgLsyg107d4xqW0aRn3NyMOqYS3NT2FJzjHlll54UbN+RJqYUu+L2/TZ6v4YGu1LqB8B04Gat9bCumqmursbpHP5fS4/Hg9vtHvbrEtmFjrnbcpyHV2/DkTWJeVWFo9rHa1uOEQg28LGbFo6LOanl55wcjDzmXQ17eG7DYebMnX/R6aU7Tns53VfP4rlTcLsrDdn3cIz0mL1e7wUbxIZ1xSilHgTcwK1a69gNqhYXdPncMtJdKbzuOT7qbW3c1UhhbirTK3IMqEyIsTd9Yg7+QJBjp05f9HlHGsLTYE8pyx6LssaEIcGulJoN/AtQBmxUSu1QSq0xYtti6Jx2G1fMn8DGXQ2jOona2+9jm27i8jmlCbvQgBBTI0F95MTF1y840tAFmCvYDemK0VrvASQBxoGr3RW8vOkoG3c1cO3ikY1m2bL3FP5AkGVzEnuGO5HcSvLTcTlsHG64VLB3UpDtIivdPBfgyZWnJjNrSh4l+WmsG8ViA5tqGsnNdJpm6JdITlarhcmlWWda5BdyuKGTKRPM01oHCXbTsVgsLJtTxu7aFrr7fMN+fSAYYseBZtwzirFa5UOYSGxTJmRzpKGTUOj86wT7/EHqm7qZXJo1xpXFlgS7CS2tLiUQDI1oxfZD9R109/lYoEY3qkaI8WBicSa9/X7aL3AF6qm2HoLBEOVF5rqyWoLdhKom5ZKT6RzRuqjbdBMWC8ybLsEuEl9ZZCqME83d5328obnnrOeZhQS7CdmsFhbPLMazv4lAYHiLcGzXTUybkG2aK/BEciuPBnvTBYK9JXx/WYEEu0gAC6qK6PP6qa3vGPJrBnwBDhxrZ26ltNaFORTkpOJIsV60xZ6RajfViBiQYDetudPDiwXsPDj0xQYOHu/AHwgx0yQz3AlhtVooK8y4cLC3dJty5lIJdpPKznAyuTSLnQeHPif1/jpzzXAnBEBZYToNFwz2HkoL0se4otiTYDexedML2VfXxoAvMKTn76tro6wgXfrXhakU56XT1N53zpDHQCBIa2c/RXlpcaosdiTYTax6Wj4+f5BD9Re/8g7Cs0PuP9p2Zh5rIcyiIMeFzx+ks3vgrPs7ur0EgyEKsl1xqix2JNhNLDor44Hj7Zd87qm2Xjq7B5gxKf4zOQphpMKccIu8paPvrPujXxfkpI55TbEmwW5ieVkuCrJdHDh26WCPXnY91WSXVgtRGAnu5vcGe2d4nnYJdpFwpk/M5eCxSw95rGvoxGKBSSXmurRaiGhwv7fF3hr5Oj9bgl0kmOkVOTS29nC6d+CizzvS2EVpfjouZywW1RIifrIzHNhTrOd2xXT240ixkplmj1NlsSPBbnLRfvZLtdrrGrpMNR+1EFEWi4WC7NTzttjzc1JNueaABLvJRfvM6xovPDKmz+unsbWHKWXSDSPMqSAn9Zw+9uaOPgpM2A0DEuyml5nmIDfTydGTF14e7Ghj+MSp2aYuFSIqN8tJx3tmeOw47SU3y5zXbBjWoaqUqgJ+A+QDrcBdWuuDRm1fjNzEksyLrvt4PPLYRDlxKkwqJ8NJR/fZwd7Z4zXtxXhGtth/Cjyqta4CHgV+ZuC2xShMLMmi/tRpgsHzLzbQ2NqDzWqhKNecH0uFyMpw0Of14/OHr8L2+QP09vvJNtnkX1FGLWZdBCwEnojc9QSwUClZrWE8mFicSf9A4Jw+xqiGlh5K8tOw2aRnTphTTqRlHr36tKsnfJslLfaLqgBOaK0DAJHbhsj9Is4qijOBv3a5vFdjcw+lJpuPWojBstLDAR7tjokGvFlb7ONm0HJNTc2IX+vxeAysJDEM55i7esMfP7fs2I+l9+xFrkOhEPVNXRRmBMb993G81xcLcszGONkcDnTP9j10nnJxqDF81enJE3V4fMNfacxoRh+zUcF+HJiglLJprQNKKRtQFrl/SKqrq3E6h/+xyOPx4Ha7h/26RDbcYw4GQ/z4z3/GmVGA2z37rMfaT/cz4D/BvFlTcLunGl2qYeTnnBxidcylzd089uprFJZOxO2uoHtbPdDC4oVzznyijZeRHrPX671gg9iQrhitdROwA7gzctedwHat9dAnAxcxY7VaKMpN41RbzzmPnWzpBTDlnNRCRGVGulyiV2Cf6WOXrphLug/4jVLqG0A7cJeB2xajVJyfxsnW3nPuj16NV2jCiZCEiEqLTJXR2+cDoKc/fJuear7pBMDAYNda7weWGLU9YaySvDT00XNneWztik6EZL45qYWIstmspDpt9PT7Aejp8+Gw20gx6Ugwcx6VOEdxXjo9fT66Iy2WqNbOfhx2m2lbLkJEpbns9EZa6n1eP+mucTN2xHAS7EmiOLL8V1Pb2d0xrZ395Ge7TDkRkhCDpafazzRsevp8pLnM25iRYE8S0a6Wtq7+s+5v7eyTbhiRFNIHtdh7+/2kSYtdJLrcrAsFez/5WXLiVJhfmivlTB97b7+PdGmxi0SXF5nFrn1QsIdCIdq6+inIkRa7ML90l33QqBg/qdJiF4nOnmIjM81B66Bg7+oZwOcPkiddMSIJpKfazwxzlBa7MI28LOdZLfbo/NR5WRLswvzSXCn09EW7YvykpUqLXZhAbpaL9q6/zkkdvfouO92cM9wJMVh6qh1/IEj/gJ8+r580p7TYhQlkpzvPhDn8NdgzTXpZtRCDOR024K8zO6aaeOF2CfYkkplup6t3cLCHW+9mnS9DiMFcjnCQn440aJx288afeY9MnCMrzUFPn49AIAgMarGnSbAL83Pawy32aOPGHvnajCTYk8hfZ7gLjwzo6hkgzZWCPUXeBsL8XJGumGiL3SHBLswg6zxTl0o3jEgW0a6Y6CdVh4kbNOY9MnGOaJdL9I0twS6SidMZabH3SotdmEi0KyYa7N19A2SkSrCL5HBOi11OngozyEo7uyum1+SXVQsxWLTrpScyrYC02IUpRGez6/P6z9ymmXgsrxCD2VPCQR6duteRYt5gH/VvtVLqUeBawAt0A1/WWm8d7XaF8aIXZESDXVrsIplEu17+2mI3b7vWiCP7CzBHaz0P+A7wewO2KWLAZrPiSLHS2+8nGAyZ/rJqIQaLDuuVFvsQaK3/POjLd4BypZRVax0c7baF8dJcdvq8fvoH/JGvpcUukkO0KyYZ+tiN/q3+IvDCSEK9pqZmxDv1eDwjfm2iGukxW/BzouEU724JTyfQdOoEHk+nkaXFjPyck0OsjjkUCgFwOjKVxp49u3CNk+4Yo4/5ksGulNoGTLzAw8Va60DkeXcAHweuHEkh1dXVOJ3Dn2XQ4/HgdrtHssuENZpjzl23jtT0VCqrZgKNzKyahntBubEFxoD8nJNDrI/Z8XQjA/5wu3PJYjcptvgH+0iP2ev1XrBBfMlg11ovvNRzlFIfAh4ErtVanxp2hWLMpLpS6PP6z6z9aOYFfYV4L7vdxoA/iNUCNqt5F3Af9Z8rpdQq4IfA+7TWdaOuSMRUqjOFXq/vzMgYM09dKsR7RceyO+w2LBbzBrsRv9W/As1Br8kAAAtcSURBVAaAPyilovddq7VuNWDbwmCpzhQamv309svJU5F87IOC3cyMGBVTaEQhYmykRbpioi326GXWQiSDaJ+6mScAA7nyNOmkOsPBHj2BZOaLNIR4L5st3P1i5rnYQYI96bgcKfQPBBjwBQDzfyQVYjCbNRx5KTbz9q+DBHvSibbQeyMXadjHwXAvIcZKNNCjAW9W5j46cY7o8mDdkeGOsnqSSCbRQLeaeKgjSLAnnWjXS3evD6vVgk1a7CKJ2M602CXYhYkMnuHO7CMDhHivaItdgl2YSrTF3tPvk24YkXTOtNhN/knV3EcnznEm2Pt8Z2a7EyJZRMexS4tdmIpz0Coy0mIXySYa6BLswlTsg/vY5eIkkWSkK0aYUrQrprffj90mXTEiuaTIyVNhRoPnn7ZLi10kGWukxS7j2IWpDL6UWvrYRbKRFrswpcEtdjMv5ivE+UT72MfDykmxZO6jE+c4qytGWuwiyURb6tIVI0xFgl0kMxnHLkxpcB+7TNkrkk7k7S/DHYdIKbVSKRVQSn3RqG0K40mLXSQzC3KB0pAppTKB7wF/MWJ7InZSUiTYRfKKrl8twT40PwS+D7QYtD0RI4Pf0DJXjEhWZj95OuqVjJVSNwE5Wus/KKVWjXQ7NTU1I67B4/GM+LWJajTHbLVCMAgtzafwePoNrCq25OecHGJ5zE1NHZHbU3g83pjtZ7iMPuZLBrtSahsw8UIPA98Frh9tIdXV1TidzmG/zuPx4Ha7R7v7hDLaY7b/oRHvQICyslLc7pkGVhY78nNODrE+5t2Ne2BfLSXFJbjds2O2n+EY6TF7vd4LNogvGexa64UXekwpdQVQCmxWSgEUADcrpfK01t8edqViTJyZ4c5i7o+jQlyI2d/6o+qK0Vq/BRRFv1ZK/RrYqrX+ySjrEjFkibyrrSZfqV2I97KYPdEjZFhEEoqeN7ImyZtciKjoW97sAT/qk6eDaa0/ZeT2RGyEQuFbsw/5EuJCTJ7r0mJPZmYf8iXEe0Vb6mZvsUuwJ6FIg126YkTSsbzn1qwk2JNRpC9GWuwi6SRJskuwJ6Foi1362EWyiX5KtZg82SXYk1D05Km02EWySZZ3vAR7EpM+dpFsQpd+iilIsCcl6WMXyc3sbRoJ9iQkXTFCmJsEexKTrhghzEmCPQmdGccuLXYhTEmCPQmFguFol+GOQpiTBHsSkxa7EOYkwZ6EpCtGCHOTYE9CZ0bFyMlTIUxJgj2JSYtdCHOSYE9KkZOn0mIXwpQMWWhDKfUl4AuAD/BrrRcYsV0RG8FoV4wsjSeSTChJ5hQYdYtdKXUb8BFgsdZ6DnDTqKsSY0L62EWyMvs734gW+z8BX9danwbQWp80YJsilkIyjl0IMzOij30WsFQptVEptVUp9RkDtiliKNoVY5OuGCFM6ZItdqXUNmDiBR4uBmxABXAFUAC8rZTSWuv1wymkpqZmOE8/i8fjGfFrE5URx3xA76eryWFANWNDfs7JIZbH3NjYCcCJhgY8np6Y7We4jD7mSwa71nrhxR5XSh0DntBaB4EmpdSrwGXAsIK9uroap9M5nJcA4W+I2+0e9usS2aiPeXU9APPmVlNelGlQVbElP+fkEOtj1i37oUZTVlaG2z0jZvsZjpEes9frvWCD2IiumNXAjQBKqXRgBbDTgO2KGEuxyWhXkVxkVMzQPQJUKKX2AJuB32mtXzVguyLG7CkS7CI5mf3s0qhHxWit+4BPGlCLGGPSYhfCnOQ3O4lJi10Ic5Lf7CQmLXYhzEl+s5OYTYJdCFOS3+wkJleeCmFOEuxCCGEyEuxCCGEyEuxJaH5VYbxLEELEkCHzsYvE8q17l+ILBONdhhAiRiTYk5DNZpURMSIphUiOOQXkt1sIkXxMvsiMBLsQQpiMBLsQQpiMBLsQIvmYfP5eCXYhRNKwmH7C3jAJdiFE0pBRMUIIYVYyKkYIIUQiGfUFSkqpKuC/gRzACfxea/2t0W5XCCHEyBjRYn8I+IPWej6wGLhHKXWZAdsVQggxAkYEewjIjvw/LfJ1kwHbFUIIYyXHuVNDgv3vgY8ppU4AdcD3tdZ1BmxXCCHECFhClxior5TaBky8wMPFwANAu9b6+0qpUmAdcJfW+t2hFODxeCYDR4ZasBBCjNTruzpZX3OalXOyWDknK97lGGWK2+2uG3zHJU+eaq0XXuxxpdTfAVMjz21USr0OXAkMKdijqqurcTqdw3kJAB6PB7fbPezXJTI55uQgx2y8fc37oOY0ZWVluN0qZvsZjpEes9frpaam5ryPGdEVcwS4EUAplQmsAM6/NyGEEDFnRLB/CrhPKbWTcCv9Ka31XwzYrhBCiBEY9Th2rbUHWGZALUIIEVsyKkYIIczJ5DMKSLALIYTZSLALIYTJSLALIZKOydfZkGAXQiQRk/etR0mwCyGSh8lb6lES7EKIpCOjYoQQQiQUCXYhRNKYPTUfgJmT8+JcSWyN+spTIYRIFAtUEb9/8P2kuezxLiWmpMUuhEgqZg91kGAXQgjTkWAXQgiTkWAXQgiTkWAXQgiTkWAXQgiTkWAXQgiTGQ/j2G0AAwMDI96A1+s1rJhEIcecHOSYk8NIjnlQZtre+5glFOf5Kz0ezxXAhrgWIYQQiWuF2+1+a/Ad46HFvgVYATQCgTjXIoQQicIGlBLO0LPEvcUuhBDCWHLyVAghTEaCXQghTEaCXQghTEaCXQghTEaCXQghTEaCXQghTEaCXQghTGY8XKA0YkqpOqA/8g/gq1rrl+NWUIwppVzAI8B1hI/5Ha31Z+NbVewopSYDzwy6KwfI0lqbesFKpdQq4AHAQrjx9S2t9Z/iW1VsKaU+QPiY7UAb8Cmt9ZH4VmUspdQPgNuBycAcrXVN5P4q4DdAPtAK3KW1PjiafSV0sEd8OPoNSgIPEQ70Kq11SClVHO+CYklrXQfMj36tlPoR5njPXpBSygL8Flihta5RSs0F3lZKPaO1Dsa5vJhQSuUSDrZlWusDSqm/Af4LuDG+lRnuGeDfOXcKlZ8Cj2qtfxc59p8B14xmR9IVkyCUUhnAXcDXtdYhAK31qfhWNXaUUg7gE8Bj8a5lDASB7Mj/c4BGs4Z6RCVwSmt9IPL1i8D7lFIFcazJcFrrt7TWxwffp5QqAhYCT0TuegJYqJQqHM2+zBDsjyuldiml/lMplRPvYmJoGuGPad9USm1VSq1TSl0R76LG0AeBE1rrbfEuJJYif7Q/CjyrlDpKuJV3d3yrirkDQIlSanHk609EbifGqZ6xVEH4fR0AiNw2RO4fsUQP9hVa63nAYsL9kT+Jcz2xlAJMBbZrrRcBXwX+pJTKim9ZY+bTJEFrXSmVAvxv4Bat9STgZuD3kU9spqS17gQ+BjyilNoKFAEdgC+uhSWwhA726McarbUX+E9geXwriqmjgJ/IRzat9btAC1AVz6LGglKqDLgKeDzetYyB+UCZ1vptgMhtDzAzrlXFmNZ6rdb6ikij5SdAKnA4zmWNhePABKWUDSByWxa5f8QSNtiVUulKqezI/y3AHcCO+FYVO1rrFuAN4Ho4cya9CKiNZ11j5FPAC1rr1ngXMgbqgXKllAJQSs0ESoBDca0qxpRSJZFbK/BvwE+11j3xrSr2tNZNhHPrzshddxL+VN48mu0m8giDYuCPkb9wNmAvcH98S4q5+4DHlFIPE/6Y+kmtdUecaxoLnwL+Lt5FjAWt9Uml1OeBPyiloidM79Fat8WzrjHwr0qp5YADeAX4WpzrMZxS6j+A2wj/oV6rlGrVWs8m/Hv9G6XUN4B2woMkRkXmYxdCCJNJ2K4YIYQQ5yfBLoQQJiPBLoQQJiPBLoQQJiPBLoQQJiPBLoQQJiPBLoQQJiPBLoQQJvP/AUGBSJLZcCcMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "x = np.linspace(5, 10, 1000)\n",
    "y = np.log(np.sin(x) * np.cos(x) * (-2*x + x**2 + x**3 + 3) * np.tan(x))\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
    "shuffle_dataset = 1\n",
    "validation_split = .2\n",
    "\n",
    "dataset_size = len(x)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(99)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "x = x.reshape(-1, 1)\n",
    "train_loader = torch.utils.data.DataLoader(x, batch_size=100, sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(x, batch_size=100, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=2**15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2**15, out_features=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss tensor(17.6158, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(16.0616, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(16.0343, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.5832, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.6895, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(16.2700, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.8010, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(17.1494, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(17.6032, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(17.3555, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(17.1480, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.5914, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.1954, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.0665, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.1908, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.9922, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.4446, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(17.0333, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(17.1974, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(16.4834, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(16.4094, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.0195, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.5287, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.8289, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.8390, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.7510, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.9384, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(16.1832, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(17.0151, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(16.0354, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(16.1527, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.8310, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.5254, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.3950, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.6947, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.3946, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.2797, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.7325, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(16.5360, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(15.7894, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(15.8346, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.9591, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.2478, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.3603, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.1012, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.9799, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.2267, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.5444, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(16.2094, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(14.9852, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(15.7184, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.7254, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.9304, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.7080, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.9832, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.0481, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.5457, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.3421, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(15.6487, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(14.9382, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(15.0261, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.6464, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.2617, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.4859, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.4234, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.9826, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.2474, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(15.0802, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(15.0217, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(14.7349, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(14.9755, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.9673, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.8626, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.9177, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.2021, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.7597, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.0710, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.6841, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(15.2334, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(13.9476, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(14.6433, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.7601, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.5523, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.4626, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.6627, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.5764, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.3815, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(14.3575, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(14.2034, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(13.7451, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(14.2434, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.4423, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.7888, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.1831, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.4656, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.2707, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.9875, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.4299, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(13.9583, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(13.4018, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(13.9459, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.3408, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.7551, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.0486, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.9940, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.7238, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.9795, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.2833, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(13.3595, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(13.2049, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(13.4463, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.0595, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.4036, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.7433, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.4778, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.4233, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.5810, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(13.3283, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(13.3110, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(12.5807, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(13.3752, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.6465, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.4818, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.2810, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.6009, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.9895, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.9555, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.9387, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(12.6760, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(12.4273, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(12.7613, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.0332, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.9191, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.0833, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.1839, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.8067, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.9724, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.5268, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(12.7713, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(12.0180, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(12.9333, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.5973, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.4895, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.5763, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.7610, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.8419, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.7717, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(12.1175, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(11.9253, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(11.9419, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(12.3482, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.3011, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.5201, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.3010, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.5413, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.7908, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.5034, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.9194, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(11.6937, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(11.5908, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(11.6371, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss tensor(10.5046, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.7478, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.6317, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.3995, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.0134, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.0153, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.6573, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(11.6161, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(11.3496, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(11.7369, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.8180, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.6793, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.9464, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.4746, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.0954, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.0676, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(11.4734, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(11.4476, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(10.9989, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(11.7286, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.9020, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.8142, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.9361, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.3389, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.6594, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.7542, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.7978, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(10.7733, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(10.9831, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(11.3718, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.0421, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.0814, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.7892, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.6706, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.3613, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.6912, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.9791, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(10.4520, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(10.3986, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(11.0001, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.9896, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.9942, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.9768, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.3668, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.9416, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.1635, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.8928, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(10.4963, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(9.9558, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(11.1298, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.1781, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.7079, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.5542, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.8747, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.6416, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.5436, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.9784, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(10.2436, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(9.9280, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(10.9660, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.3695, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.4467, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.6406, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.6930, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.2167, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.5012, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.5475, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(9.9527, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(9.6694, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(10.7318, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.8043, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.0960, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.7174, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.5524, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(10.1916, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1686, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.8667, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(9.4083, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(9.9820, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(10.6041, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.3688, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.7039, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.3643, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.0524, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.6278, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1450, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.6265, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(9.2639, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(9.0292, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(9.8312, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1481, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.0193, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.7039, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.4196, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.3636, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1656, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.7508, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(8.8745, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(9.1137, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(9.6876, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.5847, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.5379, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.6168, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1251, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.1132, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.0129, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.9168, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(8.6899, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(9.0595, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(9.5707, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.3307, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.4241, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1279, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.7665, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(9.7623, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.3729, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.7139, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(8.7062, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(8.4173, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(9.4610, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.4188, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.8012, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.9390, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.6235, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.9449, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.4766, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.7825, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(8.6850, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(8.6073, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.8410, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.1059, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5941, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.6670, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.9747, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.7562, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.9242, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.5044, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(8.2765, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(8.2330, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(9.3872, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.4696, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5742, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.0138, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.9123, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.8707, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.9169, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.0165, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(8.3546, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(8.1311, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.8185, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.8066, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5058, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.0011, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.4669, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.7838, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.9532, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.2406, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(7.6272, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(8.2280, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(9.3029, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.7170, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.0535, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss tensor(7.4007, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.4715, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1275, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3046, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1548, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(7.8913, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(7.9117, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(9.1524, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5348, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1103, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.2273, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.8612, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.3389, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.7955, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.5499, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(7.5027, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(8.0384, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.6272, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5800, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.7116, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.1930, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1797, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.1064, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1818, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.0051, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(7.1809, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(7.9950, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.5528, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1172, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0176, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.2772, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.6456, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(8.6052, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2580, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.5740, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(7.1705, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(7.0854, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.7227, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9631, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9825, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.9604, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3304, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.6917, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9967, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.3435, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(6.9134, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.9769, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.9803, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6245, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9521, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.6382, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3681, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.7451, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0144, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.4118, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(6.7343, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(7.2805, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.4705, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3314, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1036, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.8094, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6898, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.5763, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0856, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5653, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(6.5432, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(7.1023, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.2802, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6113, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5934, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2455, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3131, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.4044, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9070, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.1260, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(6.0668, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(7.2638, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.0405, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3356, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4100, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3281, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9684, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.2784, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8524, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.1432, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(6.8184, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.6322, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.2739, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7695, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6172, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9607, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8548, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.3193, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4261, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.7815, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(6.0702, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.4041, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.4707, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0579, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1227, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0474, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6911, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.6034, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0101, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.9415, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.9759, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.5778, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.5792, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3622, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7169, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8555, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7640, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.7697, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2033, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.6894, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(6.1625, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.3944, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.4456, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6747, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1374, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8830, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6489, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.7473, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0572, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.7871, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(6.0170, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.4400, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(8.3593, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5616, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7090, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3794, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5149, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.7490, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2438, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5148, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.6632, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.1816, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.5356, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8715, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.5608, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1441, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6122, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5074, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0507, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.8249, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.4239, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.3499, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.5463, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7601, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0453, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8519, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3252, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.5361, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3960, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.4575, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.2986, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.3313, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.7506, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9393, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0499, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0328, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2046, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss tensor(7.3962, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7965, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3762, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.0756, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.4100, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.3548, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7643, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1777, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8968, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6002, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9555, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9986, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2807, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.8730, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.5903, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.6033, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7755, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3623, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3413, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4065, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(7.2023, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8435, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3029, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.5242, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.6229, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.3496, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7816, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7769, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5855, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6410, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.6484, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9941, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2058, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.3979, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.4683, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.3628, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7842, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3760, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0110, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9883, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.9721, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9045, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8711, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.0613, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(6.0436, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.8196, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9031, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4411, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6283, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3779, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.8662, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3248, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1948, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.7543, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.9416, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.1277, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4830, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9449, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7557, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8520, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.8509, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0791, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5755, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.8523, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.5073, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.0068, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1358, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8190, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8796, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4353, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5351, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.5920, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2503, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.0787, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.7270, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.4042, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9563, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0432, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1559, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6801, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.4219, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1619, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3581, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.8176, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.3746, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.2522, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1912, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1843, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3277, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1253, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3172, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4968, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0391, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.5625, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.4020, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.0623, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0245, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3573, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7001, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9088, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3405, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1060, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8024, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.9853, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.0049, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.7331, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1850, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6053, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4392, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6693, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.4583, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.5410, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8589, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.8792, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.3833, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.9828, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0219, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8151, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1669, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8335, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.4125, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.5668, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1961, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(5.2247, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.2987, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.2732, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4033, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2441, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1351, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2309, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0160, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4282, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6900, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.4842, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.4141, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.2642, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3885, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5696, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0282, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2162, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2653, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1336, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1762, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.3332, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.1788, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.7937, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9247, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5392, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0128, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2213, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.4240, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0205, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5400, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.0850, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.4257, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.7924, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4807, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4962, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0767, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0090, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0653, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1098, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3281, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss tensor(4.9258, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.7695, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.3791, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5789, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3573, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1885, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4310, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1179, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2300, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7238, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.5251, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.1766, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.2247, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0609, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0171, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0913, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6455, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0392, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1082, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5518, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.6317, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.1997, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.0621, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9025, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4974, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2843, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2722, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3523, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8654, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6071, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.6355, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.8725, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.2717, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1218, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6166, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2022, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2811, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7999, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.7941, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0652, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.0579, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.0616, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.7056, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3317, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6411, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2249, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1703, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2608, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9056, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1204, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.8495, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.9253, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.0685, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.7980, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4766, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1647, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.5647, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.6115, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9600, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7535, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.7462, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.6540, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.2756, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0338, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.2002, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0742, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9380, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0022, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9446, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9185, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.2268, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.2820, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.5322, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.2721, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4465, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2467, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9342, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3823, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3413, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2577, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.9830, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.9767, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.7926, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1833, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9416, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1647, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0282, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6607, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5384, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3835, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.2077, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.7958, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.4819, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8721, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8747, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8956, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3276, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1067, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5691, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3881, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.9445, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.5510, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.6113, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6823, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3209, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6164, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1375, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.0299, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6127, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7378, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.2448, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.3384, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.4044, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4348, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6967, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2953, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1565, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.5378, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1110, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8965, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.1807, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.0022, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.9042, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.7926, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.0935, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4000, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4954, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2786, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4632, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6334, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(4.1633, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.4356, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.2002, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4053, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.2232, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1127, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1708, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2435, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.2850, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6294, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.8249, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.7234, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.5950, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6080, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4104, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1489, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1466, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2805, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5668, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7795, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.6293, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.8942, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.6273, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8822, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5943, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4803, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2725, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4807, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9191, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6501, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.6478, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.7138, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.8769, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss tensor(3.6718, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.0619, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0524, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2507, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5473, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6265, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3267, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.6517, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.6824, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(5.7317, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4427, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.0345, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4757, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9518, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1274, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3379, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2046, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.5248, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.2766, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.0803, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8762, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(2.9487, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3052, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9233, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7367, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4141, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2746, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.6124, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.8532, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(5.8083, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9412, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.2666, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0165, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.5015, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9379, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0672, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7438, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.4065, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.1497, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.4963, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4319, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.2337, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8377, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2512, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2774, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6906, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9025, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.8012, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.4049, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.4079, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4402, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(2.9153, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2739, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6574, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.8825, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.7718, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6648, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.6159, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.7307, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.8544, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3895, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4271, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7876, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0211, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2204, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0494, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6908, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.5688, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.6786, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.9220, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8435, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.1584, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3428, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4398, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9835, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8550, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6066, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.4416, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.4210, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.0390, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6449, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3223, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3249, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.6614, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7361, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3057, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1948, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.5044, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.3273, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.2879, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6387, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(2.7640, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2176, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.5147, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.3756, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.7549, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7008, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.9233, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.3965, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(7.0105, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.1863, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3466, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1034, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8196, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7890, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4146, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4322, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.5645, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.5322, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.6972, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5866, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3939, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1459, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3113, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2722, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.9701, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.1559, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.8811, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.5790, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.3814, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3284, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.0517, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0514, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8768, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7559, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.7881, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4017, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.3692, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.6985, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.9016, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.1760, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(2.8891, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.0178, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.3186, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.6189, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.2305, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8643, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.5076, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.0406, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.8731, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.2642, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4895, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.5283, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.1556, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.7686, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5946, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.9138, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.8963, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.3894, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(5.9208, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8129, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.1252, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.4301, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7001, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.8087, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5660, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.5091, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.9534, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.0794, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.0566, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.6435, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss tensor(3.3877, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8109, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.7489, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.3211, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.1325, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.4446, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.1421, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.1994, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(6.9966, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3473, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.5839, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.1439, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.7191, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.9345, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.0859, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.8029, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(2.9960, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(5.1175, grad_fn=<MseLossBackward>)\n",
      "train loss tensor(5.8649, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4552, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.4249, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(5.2061, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.8362, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(6.2444, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(3.3593, grad_fn=<MeanBackward0>)\n",
      "train loss tensor(4.7254, grad_fn=<MeanBackward0>)\n",
      "val loss tensor(3.5333, grad_fn=<MseLossBackward>)\n",
      "val loss tensor(4.4502, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "criterion, optimizer = nn.MSELoss(), optim.Adam(net.parameters(), lr = 1e-7)\n",
    "running_loss = .0\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    net.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        label = y[train_indices[batch_idx*100 : (batch_idx+1)*100]]\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "        \n",
    "        data = Variable(data, requires_grad=True)\n",
    "        label = Variable(label, requires_grad=True)\n",
    "        \n",
    "        output = net(data).view(-1)\n",
    "        loss = criterion(output, label)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('train loss', loss)\n",
    "    losses.append(total_loss/len(train_loader)) #batch가 10개 라면, 총 10개 loss의 평균\n",
    "    \n",
    "    net.eval()\n",
    "    val_total_loss = 0\n",
    "    for batch_idx, data in enumerate(validation_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = torch.tensor(data, dtype=torch.float)\n",
    "        label = y[val_indices[batch_idx*100 : (batch_idx+1)*100]]\n",
    "        label = torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "        outputs = net(data).view(-1)\n",
    "        loss = criterion(outputs, label)\n",
    "        val_total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('val loss', loss)\n",
    "    val_losses.append(val_total_loss/len(validation_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7ea3630a58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ver2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "\n",
    "def f_x(x):\n",
    "    return x**2 + x**3\n",
    "\n",
    "def make_dataset():\n",
    "    data = []\n",
    "    for i in range(1, 100):\n",
    "        data.append((i, f_x(i), 1))\n",
    "    for j in range(100, 201):\n",
    "        data.append((j, f_x(j), 0))\n",
    "    column_names = ['x', 'f_x', 'is_f_x']\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    return df\n",
    "    \n",
    "df = make_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=2**15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2**15, out_features=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNet2, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_conda",
   "language": "python",
   "name": "torch_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
